# Choix d’architecture Data – Synthèse et justification

## 1. Contexte et objectif

L’objectif est de définir une architecture de stockage et d’analyse pour des données de transport, en privilégiant la simplicité, la robustesse et la traçabilité.

Le périmètre couvre :

* des données structurées issues de fichiers CSV et GTFS,
* des traitements analytiques batch,
* une chaîne de données courte : brut → final,
* l’absence volontaire de couche de staging intermédiaire.

Cette approche vise à réduire la complexité opérationnelle tout en garantissant la rejouabilité des traitements et l’évolutivité de la solution.

---

## 2. Choix des formats de données

### 2.1 CSV pour les données tabulaires

Le format CSV est retenu pour le stockage des données tabulaires brutes.

Justifications :

* format ouvert, standard et largement répandu,
* compatibilité native avec Pandas et PySpark,
* facilité d’ingestion, de lecture et de débogage,
* absence de dépendance à un moteur ou à un schéma imposé.

Les limitations du CSV (absence de typage fort, performances moindres) sont acceptées car ce format est utilisé uniquement pour le stockage des données brutes, et non pour l’analyse finale.

---

### 2.2 GTFS fourni en archive ZIP

Les données de transport sont fournies sous forme de GTFS, encapsulé dans une archive ZIP.

Justifications :

* GTFS est un standard métier imposé par l’écosystème du transport public,
* l’archive ZIP garantit la cohérence d’un snapshot de données à un instant donné,
* réduction du volume stocké et du nombre de fichiers,
* conservation stricte du format original fourni par la source.

Les fichiers sont décompressés à la volée lors des traitements, sans stockage intermédiaire.

---

## 3. Stratégie de stockage

### 3.1 Stockage des données brutes

Les données brutes sont stockées telles quelles, sans transformation.

Principes retenus :

* immutabilité des données sources,
* conservation de l’intégralité de l’information,
* possibilité de rejouer les traitements à tout moment,
* traçabilité et auditabilité complètes.

---

### 3.2 Stockage des données finales

Les données finales correspondent aux données transformées, nettoyées et enrichies, prêtes à être exploitées pour l’analyse.

Caractéristiques :

* schéma maîtrisé et cohérent,
* données consolidées et exploitables,
* formats optimisés possibles (ex. Parquet).

---

### 3.3 Absence de couche de staging intermédiaire

Le choix a été fait de ne pas introduire de couche de staging intermédiaire.

Justifications :

* complexité technique et opérationnelle inutile,
* duplication des données sans valeur métier ajoutée,
* risques de désynchronisation entre les couches,
* surcoûts de maintenance.

La logique de staging est remplacée par :

* des traitements déterministes,
* un stockage brut immuable,
* une reconstruction systématique de la couche finale.

---

## 4. Choix des moteurs d’analyse

### 4.1 Pandas

Pandas est utilisé pour :

* l’exploration des données,
* les analyses sur des volumes modérés,
* le prototypage rapide.

Avantages :

* simplicité d’utilisation,
* rapidité de développement,
* bonne lisibilité du code.

---

### 4.2 PySpark

PySpark est utilisé pour :

* le traitement de volumes de données plus importants,
* les jointures et agrégations complexes,
* les traitements batch distribués.

Avantages :

* scalabilité horizontale,
* lecture directe des formats CSV et GTFS,
* continuité naturelle entre ingestion et transformation.

---

## 5. Justification comparée avec les solutions SQL et NoSQL

### 5.1 Bases SQL (SQLite, PostgreSQL, MySQL)

Les solutions SQL ont été évaluées mais non retenues pour le stockage des données brutes.

Avantages :

* schéma structuré et contraint,
* langage SQL standardisé,
* facilité de requêtage pour des usages transactionnels.

Limites dans ce contexte :

* rigidité du schéma incompatible avec des données brutes évolutives,
* coûts de chargement et de maintenance,
* faible pertinence pour le stockage de fichiers sources,
* complexité inutile pour des traitements analytiques batch.

Les bases SQL restent pertinentes pour des usages avals (exposition, BI, API), mais pas comme socle de données brutes.

---

### 5.2 Bases NoSQL (MongoDB, équivalents)

Les solutions NoSQL ont également été évaluées.

Avantages :

* flexibilité du schéma,
* gestion de documents semi-structurés.

Limites dans ce contexte :

* surcoût opérationnel et infrastructurel,
* complexité supplémentaire pour des données déjà structurées,
* valeur ajoutée limitée face à un stockage fichier,
* intégration analytique moins naturelle que les moteurs Spark.

---

### 5.3 Stockage fichier et moteurs analytiques

Le stockage fichier couplé à Pandas et PySpark a été retenu.

Avantages :

* séparation claire entre stockage et calcul,
* simplicité d’architecture,
* excellente compatibilité avec les traitements analytiques,
* scalabilité progressive,
* coûts maîtrisés.

---

## 6. Architecture cible

Sources de données
  └── CSV / GTFS (ZIP)
        └── Stockage BRUT
              └── Traitements Pandas / PySpark
                    └── Stockage FINAL


---
## 7. Synthèse des décisions


| Élément   | Choix retenu             | Justification                    |
| ------------- | -------------------------- | ---------------------------------- |
| Format brut | CSV / GTFS ZIP           | Standard, interopérable, simple |
| Stockage    | Fichiers                 | Léger, traçable, économique   |
| Couches     | Brut et Final            | Clarté et robustesse            |
| Staging     | Aucun                    | Complexité non justifiée       |
| Analyse     | Pandas et PySpark        | Flexibilité et scalabilité     |
| SQL / NoSQL | Non retenus pour le brut | Surcoût et rigidité            |
---
## 8. Conclusion

L’architecture retenue privilégie la simplicité, la traçabilité et la maîtrise des coûts.
Elle est adaptée à un contexte analytique orienté batch, avec des volumes évolutifs et des besoins de rejouabilité, sans introduire de sur‑complexité prématurée.
